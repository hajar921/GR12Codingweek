{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shap\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.head()\n",
    "plt.figure(figsize=(12,6))\n",
    "df.boxplot(rot=90)\n",
    "plt.title(\"Detection des outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "z_scores = np.abs(zscore(df.select_dtypes(include=[np.number])))\n",
    "outliers = (z_scores > 3).sum(axis=0)\n",
    "print(\"Nombre d'outliers detectes par case :\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(keep=False)]\n",
    "\n",
    "print(\"Lignes dupliquées :\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    duplicate_col = df.duplicated(subset=[col], keep=False)\n",
    "    if duplicate_col.any():\n",
    "        print(f\"Doublons détectés dans la colonne: {col}\")\n",
    "        print(df.loc[duplicate_col, [col]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "\n",
    "Q1 = df_numeric.quantile(0.25)\n",
    "Q3 = df_numeric.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "\n",
    "outliers_iqr = ((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "print(\"Outliers détectés par la méthode IQR :\")\n",
    "print(outliers_iqr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "print(df.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "print(df_numeric.columns)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "sns.boxplot(data=df_numeric[['Age', 'Height', 'Weight', 'NCP']])\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot of Outlier-Affected Columns\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_NCP = df['NCP'].quantile(0.25)\n",
    "Q3_NCP = df['NCP'].quantile(0.75)\n",
    "IQR_NCP = Q3_NCP - Q1_NCP\n",
    "\n",
    "\n",
    "lower_bound = max(Q1_NCP - 1.5 * IQR_NCP, 1) \n",
    "upper_bound = min(Q3_NCP + 1.5 * IQR_NCP, 5) \n",
    "\n",
    "\n",
    "df_capped = df.copy()\n",
    "df_capped['NCP'] = df['NCP'].clip(lower=lower_bound, upper=upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[(df['NCP'] >= 1) & (df['NCP'] <= 5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class_distribution = df['NObeyesdad'].value_counts(normalize=True) * 100  # Convert to percentage\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=class_distribution.index, y=class_distribution.values, palette=\"viridis\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Obesity Levels\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Distribution of Obesity Levels\")\n",
    "plt.xticks(rotation=45) \n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'NObeyesdad'  \n",
    "\n",
    "class_distribution = df[target_col].value_counts()\n",
    "\n",
    "print(\"Class Distribution:\\n\", class_distribution)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=class_distribution.index, y=class_distribution.values, palette=\"viridis\")\n",
    "plt.xlabel(\"Obesity Level Classes\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Obesity Classes\")\n",
    "plt.xticks(rotation=45)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_distribution = df['NObeyesdad'].value_counts(normalize=True) * 100\n",
    "print(class_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols.remove('NObeyesdad') \n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le \n",
    "\n",
    "target_col = \"NObeyesdad\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "from collections import Counter\n",
    "print(\"New class distribution:\", Counter(y_resampled))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=pd.Series(y_resampled).value_counts().index,\n",
    "            y=pd.Series(y_resampled).value_counts().values,\n",
    "            palette=\"viridis\")\n",
    "plt.xlabel(\"Obesity Level Classes\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Balanced Distribution of Obesity Classes After SMOTE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "correlation_matrix = df_numeric.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.8\n",
    "strong_correlations = {}\n",
    "for col in correlation_matrix.columns:\n",
    "    strong_corrs = correlation_matrix[col][(correlation_matrix[col] > correlation_threshold) & (correlation_matrix[col] < 1)].index.tolist()\n",
    "    if strong_corrs:\n",
    "        strong_correlations[col] = strong_corrs\n",
    "strong_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df.head()\n",
    "\n",
    "X = df.drop(columns=['NObeyesdad']).copy()  \n",
    "y = df['NObeyesdad']\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X.loc[:, col] = le.fit_transform(X[col]) \n",
    "    label_encoders[col] = le  \n",
    "scaler = StandardScaler()\n",
    "X.loc[:, X.select_dtypes(include=['number']).columns] = scaler.fit_transform(X.select_dtypes(include=['number']))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import streamlit as st\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "df.head()\n",
    "\n",
    "categorical_features = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "\n",
    "le_target = LabelEncoder()\n",
    "df['NObeyesdad'] = le_target.fit_transform(df['NObeyesdad'])\n",
    "X = df.drop(columns=['NObeyesdad', 'Height', 'Weight']) \n",
    "y = df['NObeyesdad']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['Age', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier()}\n",
    "\n",
    "model_performance = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    model_performance[name] = accuracy\n",
    "    print(f\"Model: {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "best_model_name = max(model_performance, key=model_performance.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Accuracy: {model_performance[best_model_name]:.4f}\")\n",
    "if best_model_name == \"Random Forest\":\n",
    "    feature_importance = best_model.feature_importances_\n",
    "elif best_model_name == \"XGBoost\":\n",
    "    feature_importance = best_model.feature_importances_\n",
    "elif best_model_name == \"LightGBM\":\n",
    "    feature_importance = best_model.feature_importances_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X.columns, feature_importance)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title(f'Feature Importance for {best_model_name}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if pd.api.types.is_integer_dtype(col_type):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        elif pd.api.types.is_float_dtype(col_type):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif pd.api.types.is_object_dtype(col_type):\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def demonstrate_memory_optimization(df):\n",
    "    before_memory = df.memory_usage(deep=True)\n",
    "    print(\"Memory usage before optimization:\")\n",
    "    print(before_memory)\n",
    "    df_optimized = optimize_memory(df)\n",
    "    after_memory = df_optimized.memory_usage(deep=True)\n",
    "    print(\"\\nMemory usage after optimization:\")\n",
    "    print(after_memory)\n",
    "    return before_memory, after_memory\n",
    "before_memory, after_memory = demonstrate_memory_optimization(df)\n",
    "\n",
    "before_memory, after_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import joblib\n",
    "\n",
    "\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "\n",
    "for i in range (shap_values.shape[2]):\n",
    "    print(f\"Shape of SHAP values for class {i}: {shap_values[:,:,i].shape}\")\n",
    "    shap.summary_plot(shap_values[:,:,i], X_test)\n",
    "    shap.dependence_plot('Age', shap_values[:,:,i], X_test)\n",
    "\n",
    "\n",
    "MODEL_PATH = \"best_model.pkl\"\n",
    "SCALER_PATH = \"scaler.pkl\"\n",
    "ENCODERS_PATH = \"label_encoders.pkl\"\n",
    "joblib.dump(best_model, MODEL_PATH)\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "joblib.dump(label_encoders, ENCODERS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
